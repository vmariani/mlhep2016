{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mind the backend\n",
    "This seminar was meant for taito-GPU cluster and __not the Everware__ nodes you used throughout the course.\n",
    "\n",
    "- One can still run it on Everware node after changing data paths, but it will probably take longer without a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "gpuid = random.randint(0,3)\n",
    "import os\n",
    "print \"random GPU roll: \",gpuid\n",
    "os.environ[\"THEANO_FLAGS\"]=\"device=gpu%i\"%gpuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the challenge data\n",
    "* Currently we are using raw data features with no preprocessing in hope that NN figures that out\n",
    "* One may try any feature engineering he wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_csv(\"/homeappl/home/austyuzh/data/train_small.csv\",)\n",
    "n_features = df.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['target'].values==1\n",
    "X = df[df.columns[1:]].values.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN architecture\n",
    " * We take a simple NN with __3 hidden layers__:\n",
    "     * Layers must contain 500 hidden units each\n",
    "     * Layers must use __tanh__ nonlinearity\n",
    " * structure can be farther optimized, so feel free to experiment __after__ you got it working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix('input X')\n",
    "target_Y = T.ivector('target Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,batch_norm,dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = InputLayer([None, n_features],input_X,'input layer')\n",
    "\n",
    "n_hid = 500\n",
    "\n",
    "l_1 = DenseLayer(l_in,\n",
    "                num_units=...,\n",
    "                name='dense0',\n",
    "                nonlinearity=...)\n",
    "l_1 = ... #Mind that l2 should take previous layer as an input, not InputLayer\n",
    "l_2 = ...\n",
    "\n",
    "nn = DenseLayer(l_2,num_units=2,\n",
    "                name='dense out',\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_params(nn,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_out = lasagne.layers.get_output(nn)\n",
    "loss = lasagne.objectives.categorical_crossentropy(nn_out, target_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates =lasagne.updates.adadelta(loss,weights)\n",
    "train_fun = theano.function([input_X,target_Y],[loss,nn_out[:,1]],updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deterministic predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "det_nn_out = lasagne.layers.get_output(nn,deterministic=True)\n",
    "det_loss = lasagne.objectives.categorical_crossentropy(det_nn_out,target_Y).mean()\n",
    "val_fun = theano.function([input_X,target_Y],[det_loss,nn_out[:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop\n",
    "* Almost identical to the original loop from previous seminar\n",
    "* The only difference is that now we keep track of NN performance across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_auc_curve = []\n",
    "train_acc_curve = []\n",
    "val_auc_curve = []\n",
    "val_acc_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size=2000\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_err = 0\n",
    "    Ypred_batches = []\n",
    "    Ytrue_batches = []\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        err, y_pred = train_fun(inputs, targets)\n",
    "        \n",
    "        Ypred_batches.append(y_pred)\n",
    "        Ytrue_batches.append(targets)\n",
    "        \n",
    "        train_err += err\n",
    "        train_batches += 1\n",
    "    \n",
    "    Ypred_train = np.concatenate(Ypred_batches)\n",
    "    Ytrue_train = np.concatenate(Ytrue_batches)\n",
    "    train_acc = accuracy_score(Ytrue_train, Ypred_train>0.5)\n",
    "    train_auc = roc_auc_score(Ytrue_train, Ypred_train)\n",
    "    \n",
    "    train_acc_curve.append(train_acc)\n",
    "    train_auc_curve.append(train_auc)\n",
    "\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    Ypred_batches = []\n",
    "    Ytrue_batches = []\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, y_pred = val_fun(inputs, targets)\n",
    "        Ypred_batches.append(y_pred)\n",
    "        Ytrue_batches.append(targets)\n",
    "        \n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "\n",
    "    Ypred_val = np.concatenate(Ypred_batches)\n",
    "    Ytrue_val = np.concatenate(Ytrue_batches)\n",
    "    val_acc = accuracy_score(Ytrue_val, Ypred_val>0.5)\n",
    "    val_auc = roc_auc_score(Ytrue_val, Ypred_val)\n",
    "    \n",
    "    val_acc_curve.append(val_acc)\n",
    "    val_auc_curve.append(val_auc)\n",
    "\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc * 100))\n",
    "    print(\"  training AUCscore:\\t\\t{:.2f} %\".format(\n",
    "        train_auc * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc  * 100))\n",
    "    print(\"  validation AUCscore:\\t\\t{:.2f} %\".format(\n",
    "        val_auc * 100))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(val_acc_curve,label='validation accuracy')\n",
    "plt.plot(val_auc_curve,label='validation auc')\n",
    "plt.plot(train_acc_curve,label='training accuracy')\n",
    "plt.plot(train_auc_curve,label='training auc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0.6,0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "plt.title('Resulting ROC curves')\n",
    "\n",
    "fpr,tpr,_ = roc_curve(Ytrue_val,Ypred_val)\n",
    "plt.plot(fpr,tpr,label='validation ROC, auc=%.5f'%(val_auc))\n",
    "fpr,tpr,_ = roc_curve(Ytrue_train,Ypred_train)\n",
    "plt.plot(fpr,tpr,label='training ROC, auc=%.5f'%(train_auc))\n",
    "\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "Add batch normalization after each hidden layer. See if new network does better that the old one.\n",
    "\n",
    "To add Batch Normalization, one can use batch_norm:\n",
    "\n",
    "```\n",
    "normalized_layer = lasagne.layers.batch_norm(previous_layer)\n",
    "```\n",
    "\n",
    "For simplicity, one can implement batch-normalized NN in a new notebook by \n",
    "clicking __File__ -> __Make a copy__ in the jupyter notebook top bar. Do not forget to rename the copy.\n",
    "\n",
    "\n",
    "Alternatively, one can just copy-paste code or start editing\n",
    "\n",
    "\n",
    "__Bonus task 1.1__:\n",
    " - See if batch-normalizing input does you any good.\n",
    " - Which operation is this ~equivalent to in terms of Scikit-Learn?\n",
    " \n",
    " \n",
    "__ After you made it through batch_normalization, scroll to the notebook bottom for the second assignment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "#### Assuming, you made it through the batch-normalization assignment.\n",
    "\n",
    "__Task2__\n",
    "\n",
    "If you have seen the batch-normalized NN learning curves, you probably have noticed that the validation curves first ascended to a peak and than started to descend while the training score still increase. That's some __overfitting__ happening right in front of you!\n",
    "\n",
    "Your second task, and also a reasonable step towards improving your kaggle ensemble, is to get rid of that overfitting by the tools you know already:\n",
    "\n",
    "- __Early stopping__: stop training when validation score starts decreasing\n",
    "- __Dropout__: force nn to learn redundant representations using [lasagne.layers.DropoutLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html) (syntax like batch_norm)\n",
    "- __L2 or L1 regularization__: plain old weight penalty still works here. Can be implemented using [lasagne.regularization](http://lasagne.readthedocs.io/en/latest/modules/regularization.html)\n",
    "- __Distortion__: add random (e.g. gaussian) noize to the input data to virtually increase dataset size. \n",
    "   * Can be done via [gaussian noise layer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html#lasagne.layers.GaussianNoiseLayer) or manually\n",
    "   \n",
    "   \n",
    "A friendly advice:\n",
    " - You are not restricted to a single approach, however we recommend slow and methodic changes against mixing everything in random proportions right away.\n",
    " - If a method is more mathematically sound or cool than others, it doesn't mean it's bound to score better in competition.\n",
    "   \n",
    "Again, the recommended approach is to copy a notebook and solve the task in the copied version, however you may pick ane approach at your own doom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
